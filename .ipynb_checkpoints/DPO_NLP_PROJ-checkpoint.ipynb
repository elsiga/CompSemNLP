{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f2ac3d-373b-47b5-8e17-b065f50f433d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elsiga/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/elsiga/miniconda3/envs/pytorch/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('openai/summarize_from_feedback', 'comparisons')\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the GPT2 model and tokenizer\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model_name = 'facebook/bart-base'  # or 'facebook/bart-large' for a larger model\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Pad and load model + tokenizer to GPU\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)  # Move model to GPU\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def print_gpu_usage():\n",
    "    result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "    print(result.stdout.decode('utf-8'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299172c3-867b-4f39-9d99-b9e4fe063689",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "# Preprocess the dataset\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['info']['post']\n",
    "        summary1 = item['summaries'][0]['text']\n",
    "        summary2 = item['summaries'][1]['text']\n",
    "        preference = item['choice']\n",
    "        \n",
    "        # Tokenize text and summaries\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, truncation=True, padding='max_length')\n",
    "        summary1_inputs = self.tokenizer(summary1, return_tensors='pt', max_length=self.max_length, truncation=True, padding='max_length')\n",
    "        summary2_inputs = self.tokenizer(summary2, return_tensors='pt', max_length=self.max_length, truncation=True, padding='max_length')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'summary1_ids': summary1_inputs['input_ids'].squeeze(0),\n",
    "            'summary1_attention_mask': summary1_inputs['attention_mask'].squeeze(0),\n",
    "            'summary2_ids': summary2_inputs['input_ids'].squeeze(0),\n",
    "            'summary2_attention_mask': summary2_inputs['attention_mask'].squeeze(0),\n",
    "            'preference': torch.tensor(preference)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class for BART to handle our data format\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['info']['post']\n",
    "        summary1 = item['summaries'][0]['text']\n",
    "        summary2 = item['summaries'][1]['text']\n",
    "        preference = item['choice']\n",
    "        return {\n",
    "            'text': text,\n",
    "            'summary1': summary1,\n",
    "            'summary2': summary2,\n",
    "            'preference': preference\n",
    "        }\n",
    "\n",
    "# Prepare the DataLoader\n",
    "train_data = SummarizationDataset([item for item in dataset['train']])\n",
    "train_dataloader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "\n",
    "# Function to prepare inputs for BART\n",
    "def prepare_bart_inputs(text, summary, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    targets = tokenizer(summary, return_tensors='pt', max_length=150, truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    targets = targets['input_ids'].to(device)\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "\n",
    "# # Prepare dataset\n",
    "# train_data = [item for item in dataset['train']]\n",
    "# val_data = [item for item in dataset['validation']]\n",
    "\n",
    "# train_dataset = SummaryDataset(train_data, tokenizer)\n",
    "# val_dataset = SummaryDataset(val_data, tokenizer)\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8666a4e3-504b-4542-8568-0374946ec6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reward_function(logits1, logits2, preference):\n",
    "#     # Use the mean of logits as a proxy for quality\n",
    "#     score1 = logits1.mean(dim=-1)\n",
    "#     score2 = logits2.mean(dim=-1)\n",
    "    \n",
    "#     # Compute reward based on preference\n",
    "#     if preference == 0:\n",
    "#         reward = score1 - score2\n",
    "#     else:\n",
    "#         reward = score2 - score1\n",
    "    \n",
    "#     return reward.mean()\n",
    "\n",
    "# Define reward function with numerical stability\n",
    "def reward_function(logits1, logits2, preference, epsilon=1e-10):\n",
    "    probs1 = F.softmax(logits1, dim=-1)\n",
    "    probs2 = F.softmax(logits2, dim=-1)\n",
    "    \n",
    "    # Add epsilon to avoid log(0)\n",
    "    log_probs1 = torch.log(probs1 + epsilon)\n",
    "    log_probs2 = torch.log(probs2 + epsilon)\n",
    "    \n",
    "    if preference == 0:\n",
    "        reward = torch.mean(log_probs1) - torch.mean(log_probs2)\n",
    "    else:\n",
    "        reward = torch.mean(log_probs2) - torch.mean(log_probs1)\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fc3d9-42fe-47a9-b13b-3b619a1e8fc2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch Size: 2, Loss: 0.2357616424560547\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.10102081298828125\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.2595386505126953\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.12047958374023438\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.07787704467773438\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.22821807861328125\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.3718605041503906\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.12065696716308594\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.21064376831054688\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.1778125762939453\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3082084655761719\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.054584503173828125\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.013692855834960938\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.2678050994873047\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.25801849365234375\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.19888877868652344\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.3365955352783203\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.06058311462402344\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.10507583618164062\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.03508567810058594\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3165607452392578\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.16133880615234375\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3821449279785156\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.08115577697753906\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.0007343292236328125\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.14958763122558594\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.2700691223144531\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3722820281982422\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.17371368408203125\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.2756805419921875\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.3819427490234375\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.029949188232421875\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3131732940673828\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.10976600646972656\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.14459991455078125\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.10986709594726562\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.17925453186035156\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.38088417053222656\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.21260833740234375\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.10633659362792969\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.006954193115234375\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.5300235748291016\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.1016998291015625\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.2359619140625\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.21624755859375\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.08873748779296875\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.1418895721435547\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.04945564270019531\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.2232532501220703\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.4065361022949219\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.3470878601074219\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.6158237457275391\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.06987380981445312\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.06987953186035156\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.38329124450683594\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.009395599365234375\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.4173736572265625\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.169097900390625\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.6192474365234375\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.09459304809570312\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.26866912841796875\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.035625457763671875\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.212982177734375\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.03756904602050781\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.42767906188964844\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.41068267822265625\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.5798797607421875\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.068756103515625\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.5108528137207031\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.42037200927734375\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.21781539916992188\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.31059837341308594\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3522605895996094\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.11122512817382812\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.4093437194824219\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.4004707336425781\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.5855522155761719\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.12088203430175781\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.2080535888671875\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.1664600372314453\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.016206741333007812\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.1765308380126953\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3443470001220703\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.2664070129394531\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.3802051544189453\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.02585601806640625\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.5848636627197266\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.15318870544433594\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.4607257843017578\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.0275421142578125\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.19809913635253906\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.525390625\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.5168552398681641\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.30954551696777344\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.21394729614257812\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.26539039611816406\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.05399513244628906\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3961067199707031\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.1072235107421875\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.17068862915039062\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.2251434326171875\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.45876312255859375\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.056148529052734375\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.556427001953125\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.35367584228515625\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.5311489105224609\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.4480018615722656\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.6259822845458984\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.4088096618652344\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.7758045196533203\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.16064834594726562\n",
      "Epoch: 0, Batch Size: 2, Loss: -1.1404609680175781\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.39272308349609375\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.3059425354003906\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.46302223205566406\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.23960113525390625\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.2057514190673828\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.14763450622558594\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.6319789886474609\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.38458919525146484\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.6224708557128906\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.6958580017089844\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.6266708374023438\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.1252765655517578\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.6617221832275391\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.7194099426269531\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.017238616943359375\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.09274864196777344\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.105438232421875\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.9796981811523438\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.20952606201171875\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.4273719787597656\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3027458190917969\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.21076202392578125\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.4279956817626953\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.5957221984863281\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.08050155639648438\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.3646717071533203\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.006855010986328125\n",
      "Epoch: 0, Batch Size: 2, Loss: -1.4548358917236328\n",
      "Epoch: 0, Batch Size: 2, Loss: 1.0079002380371094\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.4470634460449219\n",
      "Epoch: 0, Batch Size: 2, Loss: -1.4582481384277344\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.11800003051757812\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.5419692993164062\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.09483146667480469\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.1614055633544922\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.6064491271972656\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.2230854034423828\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.1420135498046875\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.3936748504638672\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.16342926025390625\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.22397613525390625\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.0029888153076171875\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.21147537231445312\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.33185386657714844\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.8520545959472656\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.22994613647460938\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.3814249038696289\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.22579574584960938\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.6779956817626953\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.5055446624755859\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.0796966552734375\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.40877532958984375\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.4296913146972656\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.00170135498046875\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.23768997192382812\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.49607086181640625\n",
      "Epoch: 0, Batch Size: 2, Loss: 0.3891639709472656\n",
      "Epoch: 0, Batch Size: 2, Loss: -0.11544227600097656\n"
     ]
    }
   ],
   "source": [
    "# GPT2\n",
    "# Training loop with gradient clipping and reduced learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)  # Reduced learning rate\n",
    "num_epochs = 3\n",
    "max_grad_norm = 1.0  # Gradient clipping\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        summary1_ids = batch['summary1_ids'].to(device)\n",
    "        summary1_attention_mask = batch['summary1_attention_mask'].to(device)\n",
    "        summary2_ids = batch['summary2_ids'].to(device)\n",
    "        summary2_attention_mask = batch['summary2_attention_mask'].to(device)\n",
    "        preferences = batch['preference'].to(device)\n",
    "        \n",
    "        outputs1 = model(input_ids, attention_mask=attention_mask, labels=summary1_ids)\n",
    "        logits1 = outputs1.logits\n",
    "        \n",
    "        outputs2 = model(input_ids, attention_mask=attention_mask, labels=summary2_ids)\n",
    "        logits2 = outputs2.logits\n",
    "        \n",
    "        reward = 0\n",
    "        for i in range(len(preferences)):\n",
    "            reward += reward_function(logits1[i], logits2[i], preferences[i])\n",
    "        \n",
    "        loss = -reward  # Maximize reward by minimizing negative reward\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Print GPU usage\n",
    "        print_gpu_usage()\n",
    "\n",
    "        # Print and empty cache\n",
    "        print(f'Epoch: {epoch}, Batch Size: {batch_size}, Loss: {loss.item()}')\n",
    "        torch.cuda.empty_cache()  # Empty GPU cache\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine-tuned-gpt2')\n",
    "tokenizer.save_pretrained('./fine-tuned-gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fe74fd-19d6-491f-8167-f167cdd0eee5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 372 at dim 1 (got 205)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m summary2 \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m preferences \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreference\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m inputs1, targets1 \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_bart_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m inputs2, targets2 \u001b[38;5;241m=\u001b[39m prepare_bart_inputs(text, summary2, tokenizer, device)\n\u001b[1;32m     18\u001b[0m outputs1 \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs1, labels\u001b[38;5;241m=\u001b[39mtargets1)\n",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m, in \u001b[0;36mprepare_bart_inputs\u001b[0;34m(text, summary, tokenizer, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_bart_inputs\u001b[39m(text, summary, tokenizer, device):\n\u001b[0;32m---> 63\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     targets \u001b[38;5;241m=\u001b[39m tokenizer(summary, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2803\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2801\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2802\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2803\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2889\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2884\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2885\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2886\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2887\u001b[0m         )\n\u001b[1;32m   2888\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2891\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2909\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2910\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2911\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2927\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2928\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3080\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3071\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3072\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3073\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3077\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3078\u001b[0m )\n\u001b[0;32m-> 3080\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils.py:807\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m     second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 807\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_prepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils.py:887\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    877\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m    879\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    880\u001b[0m     batch_outputs,\n\u001b[1;32m    881\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding_strategy\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    884\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    885\u001b[0m )\n\u001b[0;32m--> 887\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:764\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    760\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    761\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    762\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    768\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    769\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "# BART\n",
    "# Training loop\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        text = batch['text']\n",
    "        summary1 = batch['summary1']\n",
    "        summary2 = batch['summary2']\n",
    "        preferences = batch['preference'].to(device)\n",
    "    \n",
    "        # Prepare batched inputs for BART\n",
    "        inputs1, targets1 = prepare_bart_inputs(text, summary1, tokenizer, device)\n",
    "        inputs2, targets2 = prepare_bart_inputs(text, summary2, tokenizer, device)\n",
    "    \n",
    "        outputs1 = model(**inputs1, labels=targets1)\n",
    "        logits1 = outputs1.logits\n",
    "    \n",
    "        outputs2 = model(**inputs2, labels=targets2)\n",
    "        logits2 = outputs2.logits\n",
    "    \n",
    "        reward = 0\n",
    "        for i in range(len(preferences)):\n",
    "            reward += reward_function(logits1[i], logits2[i], preferences[i])\n",
    "    \n",
    "        loss = -reward\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "        torch.cuda.empty_cache()  # Empty GPU cache\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine-tuned-bart')\n",
    "tokenizer.save_pretrained('./fine-tuned-bart')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
