{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f109f5e8-cced-4abc-9e6c-679f5e3cc93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elsiga/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/elsiga/miniconda3/envs/pytorch/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('openai/summarize_from_feedback', 'comparisons')\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model_path = './fine-tuned-gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Load the original GPT-2 model\n",
    "original_model_path = 'gpt2'\n",
    "original_model = GPT2LMHeadModel.from_pretrained(original_model_path)\n",
    "original_tokenizer = GPT2Tokenizer.from_pretrained(original_model_path)\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "fine_tuned_model.to(device)\n",
    "original_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddef680-3902-4cc1-b46b-0c3dc0a96b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, text, max_length=45):\n",
    "    inputs = tokenizer.encode(\"summarize this following text in at most 30 words in the form a TLDR: ###\\n\"+text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length').to(device)\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(summary)\n",
    "    return summary\n",
    "\n",
    "# validation dataset from openai/summarize_from_feedback\n",
    "val_data = [item for item in dataset['validation']]\n",
    "val_text = [item['info']['post'] for item in val_data]\n",
    "# get summary that got preferred by humans. \n",
    "# print(val_text[0])\n",
    "\n",
    "# Generate summaries\n",
    "fine_tuned_summaries = [generate_summary(fine_tuned_model, tokenizer, text) for text in val_text]\n",
    "original_summaries = [generate_summary(original_model, original_tokenizer, text) for text in val_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04741bf2-b30c-45ac-bd6d-eff5c99712f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "\n",
    "def evaluate_summaries(reference_summaries, generated_summaries):\n",
    "    # ROUGE evaluation\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = [scorer.score(ref, gen) for ref, gen in zip(reference_summaries, generated_summaries)]\n",
    "\n",
    "    # BERTScore evaluation\n",
    "    P, R, F1 = bert_score.score(generated_summaries, reference_summaries, lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "    return rouge_scores, P, R, F1\n",
    "\n",
    "# Assuming we have reference summaries for evaluation\n",
    "reference_summary = []\n",
    "for i, sample in enumerate(val_data): \n",
    "    current_preferred_summary = sample['summaries'][0]['text']\n",
    "    if sample['choice'] == 1:\n",
    "        current_preferred_summary = sample['summaries'][1]['text']\n",
    "    reference_summary.append(current_preferred_summary)\n",
    "\n",
    "# Evaluate fine-tuned model summaries\n",
    "fine_tuned_rouge_scores, fine_tuned_P, fine_tuned_R, fine_tuned_F1 = evaluate_summaries(reference_summaries, fine_tuned_summaries)\n",
    "\n",
    "# Evaluate original model summaries\n",
    "original_rouge_scores, original_P, original_R, original_F1 = evaluate_summaries(reference_summaries, original_summaries)\n",
    "\n",
    "# Print ROUGE scores\n",
    "print(\"Fine-Tuned Model ROUGE Scores:\")\n",
    "for i, scores in enumerate(fine_tuned_rouge_scores):\n",
    "    print(f\"Text {i + 1}:\", scores)\n",
    "\n",
    "print(\"\\nOriginal Model ROUGE Scores:\")\n",
    "for i, scores in enumerate(original_rouge_scores):\n",
    "    print(f\"Text {i + 1}:\", scores)\n",
    "\n",
    "# Print BERTScores\n",
    "print(\"\\nFine-Tuned Model BERTScores:\")\n",
    "for i in range(len(fine_tuned_summaries)):\n",
    "    print(f\"Text {i + 1}: Precision={fine_tuned_P[i].item():.4f}, Recall={fine_tuned_R[i].item():.4f}, F1={fine_tuned_F1[i].item():.4f}\")\n",
    "\n",
    "print(\"\\nOriginal Model BERTScores:\")\n",
    "for i in range(len(original_summaries)):\n",
    "    print(f\"Text {i + 1}: Precision={original_P[i].item():.4f}, Recall={original_R[i].item():.4f}, F1={original_F1[i].item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
